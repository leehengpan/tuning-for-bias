{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "897f5b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import json\n",
    "from preprocess_v2 import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7aea63cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25184, 256) (25184, 16) (1325, 256) (1325, 16)\n",
      "Unique words in glove: 400003\n",
      "Hits: 14315; Misses: 685\n",
      "Hits: 68712; Misses: 21651\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2470)\n",
    "train_content, train_title, test_content, test_title = train_test_split()\n",
    "(content_vocab, content_word_index, content_index_word, \n",
    " title_vocab, title_word_index, title_index_word) = vectorize_data(train_content, train_title)\n",
    "\n",
    "train_content_vec = CONTENT_VECTORIZER(train_content)\n",
    "train_title_vec = TITLE_VECTORIZER(train_title)\n",
    "test_content_vec = CONTENT_VECTORIZER(test_content)\n",
    "test_title_vec = TITLE_VECTORIZER(test_title)\n",
    "\n",
    "print(train_content_vec.shape, train_title_vec.shape, test_content_vec.shape, test_title_vec.shape)\n",
    "\n",
    "glove_index = build_glove_embed_index()\n",
    "title_embedding_init, title_vocab_size = build_embedding_init(title_word_index, glove_index)\n",
    "content_embedding_init, content_vocab_size = build_embedding_init(content_word_index, glove_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e87ec5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_size, window_size, initializer, trainable=False):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.window_size = window_size\n",
    "        self.initializer = initializer\n",
    "        self.trainable = trainable\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_size, mask_zero=True,\n",
    "                                                   embeddings_initializer=self.initializer,\n",
    "                                                   trainable=self.trainable)    \n",
    "        self.positional_encoding = positional_encoding(window_size, embedding_size)\n",
    "    \n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "        \n",
    "    def call(self, x, add_positional_embedding=True):\n",
    "        length = tf.shape(x)[1]\n",
    "        if add_positional_embedding:\n",
    "            return self.embedding(x)+positional_encoding(length, self.embedding_size)\n",
    "        else:\n",
    "            return self.embedding(x)\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        \n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output = self.mha(query=x, key=context, value=context)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x, attention_mask=None):\n",
    "        attn_output = self.mha(query=x, value=x, key=x, attention_mask=attention_mask)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x, attention_mask=None):\n",
    "        attn_output = self.mha(query=x, value=x, key=x, use_causal_mask=True, attention_mask=attention_mask)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "    \n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_size, ff_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
    "                                        tf.keras.layers.Dense(embedding_size),\n",
    "                                        tf.keras.layers.Dropout(dropout_rate)])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b341557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, embedding_size, ff_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_size = embedding_size\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.self_attention = GlobalSelfAttention(num_heads=num_heads, key_dim=embedding_size,\n",
    "                                                  dropout=dropout_rate)\n",
    "        self.ffn = FeedForward(embedding_size, ff_dim)\n",
    "\n",
    "    def call(self, x, attention_mask=None):\n",
    "        x = self.self_attention(x, attention_mask=attention_mask)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, num_heads, ff_dim, vocab_size, embedding_size, \n",
    "                 window_size, embedding_initializer, embedding_trainability=False,  dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.window_size = window_size\n",
    "        self.embedding_initializer = embedding_initializer\n",
    "        self.embedding_trainability = embedding_trainability\n",
    "        \n",
    "        self.pos_embedding = PositionalEmbedding(self.vocab_size, self.embedding_size, self.window_size,\n",
    "                                                 self.embedding_initializer, self.embedding_trainability)\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(num_heads, embedding_size, ff_dim, dropout_rate) for i in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        # x is tokenized numerical values\n",
    "        mask = self.pos_embedding.compute_mask(x)\n",
    "        mask = mask[:,tf.newaxis,:]\n",
    "        x = self.pos_embedding(x)\n",
    "        \n",
    "        # Add dropout.\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, attention_mask=mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4934f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, embedding_size, ff_dim, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.causal_self_attention = CausalSelfAttention(num_heads=num_heads, key_dim=embedding_size, \n",
    "                                                         dropout=dropout_rate)\n",
    "        self.cross_attention = CrossAttention(num_heads=num_heads, key_dim=embedding_size, \n",
    "                                              dropout=dropout_rate)\n",
    "        self.ffn = FeedForward(embedding_size, ff_dim)\n",
    "\n",
    "    def call(self, x, context, attention_mask=None):\n",
    "        x = self.causal_self_attention(x=x, attention_mask=attention_mask)\n",
    "        x = self.cross_attention(x=x, context=context)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "    \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, num_heads, ff_dim, vocab_size, embedding_size, window_size,\n",
    "                 embedding_initializer, embedding_trainability=False, dropout_rate=0.1):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.embedding_initializer = embedding_initializer\n",
    "        self.embedding_trainability = embedding_trainability\n",
    "\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(self.vocab_size, self.embedding_size, self.window_size,\n",
    "                                                 self.embedding_initializer, self.embedding_trainability)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(num_heads, embedding_size, ff_dim,  dropout_rate=dropout_rate) \n",
    "                           for i in range(num_layers)]\n",
    "        \n",
    "    def call(self, x, context):\n",
    "        # `x` is token-IDs shape (batch, target_seq_len)\n",
    "        mask = self.pos_embedding.compute_mask(x)\n",
    "        mask = mask[:,tf.newaxis,:]\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x  = self.dec_layers[i](x, context, attention_mask=mask)\n",
    "\n",
    "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66b46ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(self, num_layers, num_heads, ff_dim, embedding_size, \n",
    "                 content_vocab_size, title_vocab_size, content_window_size, title_window_size,\n",
    "                 content_embedding_initializer, title_embedding_initializer,\n",
    "                 content_embedding_trainability, title_embedding_trainability, \n",
    "                 dropout_rate=0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers, num_heads, ff_dim, content_vocab_size, embedding_size, \n",
    "                               content_window_size, content_embedding_initializer, content_embedding_trainability,\n",
    "                               dropout_rate)\n",
    "        self.decoder = Decoder(num_layers, num_heads, ff_dim, title_vocab_size, embedding_size,\n",
    "                               title_window_size, title_embedding_initializer, title_embedding_trainability,\n",
    "                               dropout_rate)\n",
    "        \n",
    "        self.dense_layer = tf.keras.layers.Dense(title_vocab_size)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        content, title = inputs        \n",
    "        context = self.encoder(content)\n",
    "        output = self.decoder(title, context)\n",
    "        logits = self.dense_layer(output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3d3236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "num_heads = 8\n",
    "ff_dim = 256\n",
    "embedding_size = GLOVE_EMBED_SZ\n",
    "content_window_size = CONTENT_SEQ_LEN\n",
    "title_window_size = TITLE_SEQ_LEN\n",
    "content_embedding_initializer = tf.keras.initializers.Constant(content_embedding_init)\n",
    "title_embedding_initializer = tf.keras.initializers.Constant(title_embedding_init)\n",
    "content_embedding_trainability = True\n",
    "title_embedding_trainability = True\n",
    "dropout_rate = 0.1\n",
    "\n",
    "train_title_labels = train_title_vec[:,:,tf.newaxis]\n",
    "test_title_labels = test_title_vec[:,:,tf.newaxis]\n",
    "\n",
    "model = TransformerModel(num_layers, num_heads, ff_dim, embedding_size, content_vocab_size, title_vocab_size,\n",
    "                         content_window_size, title_window_size, content_embedding_initializer, title_embedding_initializer,\n",
    "                         content_embedding_trainability, title_embedding_trainability, dropout_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdeedca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "126/126 [==============================] - 801s 6s/step - loss: 7.5555 - masked_accuracy: 0.0944\n",
      "Epoch 2/15\n",
      "126/126 [==============================] - 795s 6s/step - loss: 6.7145 - masked_accuracy: 0.1270\n",
      "Epoch 3/15\n",
      "126/126 [==============================] - 841s 7s/step - loss: 6.1353 - masked_accuracy: 0.1574\n",
      "Epoch 4/15\n",
      "126/126 [==============================] - 865s 7s/step - loss: 5.6819 - masked_accuracy: 0.1807\n",
      "Epoch 5/15\n",
      "126/126 [==============================] - 877s 7s/step - loss: 5.3338 - masked_accuracy: 0.2009\n",
      "Epoch 6/15\n",
      "126/126 [==============================] - 873s 7s/step - loss: 5.0425 - masked_accuracy: 0.2175\n",
      "Epoch 7/15\n",
      "126/126 [==============================] - 872s 7s/step - loss: 4.7752 - masked_accuracy: 0.2336\n",
      "Epoch 8/15\n",
      "126/126 [==============================] - 875s 7s/step - loss: 4.5400 - masked_accuracy: 0.2480\n",
      "Epoch 9/15\n",
      "126/126 [==============================] - 872s 7s/step - loss: 4.3181 - masked_accuracy: 0.2631\n",
      "Epoch 10/15\n",
      "126/126 [==============================] - 867s 7s/step - loss: 4.1110 - masked_accuracy: 0.2782\n",
      "Epoch 11/15\n",
      "126/126 [==============================] - 852s 7s/step - loss: 3.9118 - masked_accuracy: 0.2945\n",
      "Epoch 12/15\n",
      "126/126 [==============================] - 838s 7s/step - loss: 3.7328 - masked_accuracy: 0.3122\n",
      "Epoch 13/15\n",
      "126/126 [==============================] - 835s 7s/step - loss: 3.5574 - masked_accuracy: 0.3313\n",
      "Epoch 14/15\n",
      "126/126 [==============================] - 831s 7s/step - loss: 3.3988 - masked_accuracy: 0.3503\n",
      "Epoch 15/15\n",
      "126/126 [==============================] - 835s 7s/step - loss: 3.2472 - masked_accuracy: 0.3704\n"
     ]
    }
   ],
   "source": [
    "model_name = 'modelv2-2blocks-8heads-256ffdim-trainableemb'\n",
    "\n",
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = tf.expand_dims(loss_object(label, pred),axis=2)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.expand_dims(tf.argmax(pred, axis=2), axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "\n",
    "    match = match & mask\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n",
    "\n",
    "model.compile(optimizer='Adam', loss=masked_loss, metrics=[masked_accuracy])\n",
    "\n",
    "model.fit(x=(train_content_vec, train_title_vec[:,:-1]), y=train_title_labels[:,1:], \n",
    "          batch_size=200, epochs=15)\n",
    "\n",
    "model_weights_path = f'../models/weights/{model_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20bea88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_model_weights(filepath):\n",
    "    if os.path.isfile(filepath):\n",
    "        confirmation = input('File exists; hit y to override: ')\n",
    "        if confirmation.lower()=='y':\n",
    "            model.save_weights(filepath)\n",
    "        else:\n",
    "            print('Not saving; try saving with different filename')\n",
    "    else:\n",
    "        model.save_weights(filepath)\n",
    "\n",
    "save_model_weights(model_weights_path)\n",
    "\n",
    "# LOAD WEIGHTS USING:\n",
    "# model.load_weights('../models/weights/modelv2-2blocks-5heads-256ffdim-trainableemb')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "480d77b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_from_ind(indexes, index_word_dict=title_index_word):\n",
    "    sentence = \"\"\n",
    "    \n",
    "    for index in indexes:\n",
    "   \n",
    "        sentence += index_word_dict[index]\n",
    "        sentence += \" \"\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "## NOT NECESSARY ANYMORE- DISCUSS AND REMOVE\n",
    "# predictions = model.predict(x=(test_content_vec[:100], test_title_vec[:100][:,:-1]))\n",
    "\n",
    "# for i in range(0,10):\n",
    "#     tokens = np.argmax(predictions[i],axis=1)\n",
    "#     true = test_title_labels[i].numpy().reshape((16,))\n",
    "#     count = 0\n",
    "#     for num in tokens:\n",
    "#         if num == 0:\n",
    "#             count +1\n",
    "\n",
    "#     # if count < 10:\n",
    "#     # print(tokens)\n",
    "#     print(f'Predicted Sentence {i}:',sentence_from_ind(tokens))\n",
    "#     print(f'True Sentence {i}:',sentence_from_ind(true))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "562432d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def text_to_title(content, model=model, output_len=TITLE_SEQ_LEN):\n",
    "    \"\"\"Converts vectorized text to title\n",
    "    Arguments:\n",
    "        content - vectorized text\"\"\"\n",
    "    \n",
    "    start, end = tf.constant(title_word_index['<start>'], dtype=tf.int64), tf.constant(title_word_index['<end>'], dtype=tf.int64)\n",
    "    start = start[tf.newaxis]\n",
    "    end = end[tf.newaxis]\n",
    "    \n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    output_array = output_array.write(0, start)\n",
    "\n",
    "    for i in tf.range(output_len):\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        predictions = model([content[tf.newaxis], output], training=False)\n",
    "        \n",
    "        # Select the last token from the `seq_len` dimension.\n",
    "        predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "        predicted_id = tf.argmax(predictions, axis=2)\n",
    "\n",
    "        # Concatenate the `predicted_id` to the output which is given to the\n",
    "        # decoder as its input.\n",
    "        output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "        if predicted_id == end:\n",
    "            break\n",
    "        \n",
    "    output = output_array.stack().numpy().reshape(1,-1)\n",
    "    predicted_title = sentence_from_ind(output[0].tolist())\n",
    "    return predicted_title\n",
    "\n",
    "true_titles = []\n",
    "predicted_titles = []\n",
    "\n",
    "for index in range(200):\n",
    "    content_vec, true_title = test_content_vec[index], test_title[index]\n",
    "    predicted_title = text_to_title(content_vec)\n",
    "    true_titles.append(true_title)\n",
    "    predicted_titles.append(predicted_title)\n",
    "    \n",
    "    if index%50==0:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f03fcad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=[true_titles, predicted_titles]).T\n",
    "df.columns = ['true_title','predicted_title']\n",
    "df.to_csv('temp2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
