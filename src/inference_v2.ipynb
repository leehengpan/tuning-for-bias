{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09540d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import json\n",
    "from preprocess_v2 import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d93b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_bias(content):\n",
    "    \"\"\"Convenience function with no generalization- just a hack to reverse the bias\"\"\"\n",
    "    words = content.split()\n",
    "    view = words[1]\n",
    "    \n",
    "    if view=='liberal':\n",
    "        words[1] = 'conservative'\n",
    "    else:\n",
    "        words[1] = 'liberal'\n",
    "    reverse_bias_content = ' '.join(words)\n",
    "    return reverse_bias_content, view, words[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0b19e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 17:43:24.614557: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25184, 256) (25184, 16) (1325, 256) (1325, 16) (1325, 256)\n",
      "Unique words in glove: 400003\n",
      "Hits: 14315; Misses: 685\n",
      "Hits: 68712; Misses: 21651\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2470)\n",
    "train_content, train_title, test_content, test_title = train_test_split()\n",
    "\n",
    "test_reverse_content = []\n",
    "test_original_view = []\n",
    "test_reverse_view = []\n",
    "\n",
    "for content in test_content:\n",
    "    reverse_bias_content, original_view, reverse_view = reverse_bias(content)\n",
    "    \n",
    "    test_reverse_content.append(reverse_bias_content)\n",
    "    test_original_view.append(original_view)\n",
    "    test_reverse_view.append(reverse_view)\n",
    "    \n",
    "(content_vocab, content_word_index, content_index_word, \n",
    " title_vocab, title_word_index, title_index_word) = vectorize_data(train_content, train_title)\n",
    "\n",
    "train_content_vec = CONTENT_VECTORIZER(train_content)\n",
    "train_title_vec = TITLE_VECTORIZER(train_title)\n",
    "\n",
    "test_content_vec = CONTENT_VECTORIZER(test_content)\n",
    "test_reverse_content_vec = CONTENT_VECTORIZER(test_reverse_content)\n",
    "test_title_vec = TITLE_VECTORIZER(test_title)\n",
    "\n",
    "\n",
    "print(train_content_vec.shape, train_title_vec.shape, test_content_vec.shape, test_title_vec.shape,\n",
    "      test_reverse_content_vec.shape)\n",
    "\n",
    "glove_index = build_glove_embed_index()\n",
    "title_embedding_init, title_vocab_size = build_embedding_init(title_word_index, glove_index)\n",
    "content_embedding_init, content_vocab_size = build_embedding_init(content_word_index, glove_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b063e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_size, window_size, initializer, trainable=False):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.window_size = window_size\n",
    "        self.initializer = initializer\n",
    "        self.trainable = trainable\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_size, mask_zero=True,\n",
    "                                                   embeddings_initializer=self.initializer,\n",
    "                                                   trainable=self.trainable)    \n",
    "        self.positional_encoding = positional_encoding(window_size, embedding_size)\n",
    "    \n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "        \n",
    "    def call(self, x, add_positional_embedding=True):\n",
    "        length = tf.shape(x)[1]\n",
    "        if add_positional_embedding:\n",
    "            return self.embedding(x)+positional_encoding(length, self.embedding_size)\n",
    "        else:\n",
    "            return self.embedding(x)\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        \n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output = self.mha(query=x, key=context, value=context)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x, attention_mask=None):\n",
    "        attn_output = self.mha(query=x, value=x, key=x, attention_mask=attention_mask)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x, attention_mask=None):\n",
    "        attn_output = self.mha(query=x, value=x, key=x, use_causal_mask=True, attention_mask=attention_mask)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "    \n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_size, ff_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
    "                                        tf.keras.layers.Dense(embedding_size),\n",
    "                                        tf.keras.layers.Dropout(dropout_rate)])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x) \n",
    "        return x\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, embedding_size, ff_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_size = embedding_size\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.self_attention = GlobalSelfAttention(num_heads=num_heads, key_dim=embedding_size,\n",
    "                                                  dropout=dropout_rate)\n",
    "        self.ffn = FeedForward(embedding_size, ff_dim)\n",
    "\n",
    "    def call(self, x, attention_mask=None):\n",
    "        x = self.self_attention(x, attention_mask=attention_mask)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, num_heads, ff_dim, vocab_size, embedding_size, \n",
    "                 window_size, embedding_initializer, embedding_trainability=False,  dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.window_size = window_size\n",
    "        self.embedding_initializer = embedding_initializer\n",
    "        self.embedding_trainability = embedding_trainability\n",
    "        \n",
    "        self.pos_embedding = PositionalEmbedding(self.vocab_size, self.embedding_size, self.window_size,\n",
    "                                                 self.embedding_initializer, self.embedding_trainability)\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(num_heads, embedding_size, ff_dim, dropout_rate) for i in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        # x is tokenized numerical values\n",
    "        mask = self.pos_embedding.compute_mask(x)\n",
    "        mask = mask[:,tf.newaxis,:]\n",
    "        x = self.pos_embedding(x)\n",
    "        \n",
    "        # Add dropout.\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, attention_mask=mask)\n",
    "        return x\n",
    "    \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, embedding_size, ff_dim, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.causal_self_attention = CausalSelfAttention(num_heads=num_heads, key_dim=embedding_size, \n",
    "                                                         dropout=dropout_rate)\n",
    "        self.cross_attention = CrossAttention(num_heads=num_heads, key_dim=embedding_size, \n",
    "                                              dropout=dropout_rate)\n",
    "        self.ffn = FeedForward(embedding_size, ff_dim)\n",
    "\n",
    "    def call(self, x, context, attention_mask=None):\n",
    "        x = self.causal_self_attention(x=x, attention_mask=attention_mask)\n",
    "        x = self.cross_attention(x=x, context=context)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "    \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, num_heads, ff_dim, vocab_size, embedding_size, window_size,\n",
    "                 embedding_initializer, embedding_trainability=False, dropout_rate=0.1):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.embedding_initializer = embedding_initializer\n",
    "        self.embedding_trainability = embedding_trainability\n",
    "\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(self.vocab_size, self.embedding_size, self.window_size,\n",
    "                                                 self.embedding_initializer, self.embedding_trainability)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(num_heads, embedding_size, ff_dim,  dropout_rate=dropout_rate) \n",
    "                           for i in range(num_layers)]\n",
    "        \n",
    "    def call(self, x, context):\n",
    "        # `x` is token-IDs shape (batch, target_seq_len)\n",
    "        mask = self.pos_embedding.compute_mask(x)\n",
    "        mask = mask[:,tf.newaxis,:]\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x  = self.dec_layers[i](x, context, attention_mask=mask)\n",
    "\n",
    "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "        return x\n",
    "\n",
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(self, num_layers, num_heads, ff_dim, embedding_size, \n",
    "                 content_vocab_size, title_vocab_size, content_window_size, title_window_size,\n",
    "                 content_embedding_initializer, title_embedding_initializer,\n",
    "                 content_embedding_trainability, title_embedding_trainability, \n",
    "                 dropout_rate=0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers, num_heads, ff_dim, content_vocab_size, embedding_size, \n",
    "                               content_window_size, content_embedding_initializer, content_embedding_trainability,\n",
    "                               dropout_rate)\n",
    "        self.decoder = Decoder(num_layers, num_heads, ff_dim, title_vocab_size, embedding_size,\n",
    "                               title_window_size, title_embedding_initializer, title_embedding_trainability,\n",
    "                               dropout_rate)\n",
    "        \n",
    "        self.dense_layer = tf.keras.layers.Dense(title_vocab_size)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        content, title = inputs        \n",
    "        context = self.encoder(content)\n",
    "        output = self.decoder(title, context)\n",
    "        logits = self.dense_layer(output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "022432d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1436f8250>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layers = 2\n",
    "num_heads = 8\n",
    "ff_dim = 256\n",
    "embedding_size = GLOVE_EMBED_SZ\n",
    "content_window_size = CONTENT_SEQ_LEN\n",
    "title_window_size = TITLE_SEQ_LEN\n",
    "content_embedding_initializer = tf.keras.initializers.Constant(content_embedding_init)\n",
    "title_embedding_initializer = tf.keras.initializers.Constant(title_embedding_init)\n",
    "content_embedding_trainability = True\n",
    "title_embedding_trainability = True\n",
    "dropout_rate = 0.1\n",
    "\n",
    "train_title_labels = train_title_vec[:,:,tf.newaxis]\n",
    "test_title_labels = test_title_vec[:,:,tf.newaxis]\n",
    "\n",
    "model = TransformerModel(num_layers, num_heads, ff_dim, embedding_size, content_vocab_size, title_vocab_size,\n",
    "                         content_window_size, title_window_size, content_embedding_initializer, title_embedding_initializer,\n",
    "                         content_embedding_trainability, title_embedding_trainability, dropout_rate)\n",
    "\n",
    "model.load_weights('../models/weights/modelv2-2blocks-8heads-256ffdim-trainableemb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62fca439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_from_ind(indexes, index_word_dict=title_index_word):\n",
    "    sentence = \"\"\n",
    "    \n",
    "    for index in indexes:\n",
    "   \n",
    "        sentence += index_word_dict[index]\n",
    "        sentence += \" \"\n",
    "    \n",
    "    return sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4418e0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/csci2470-project/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/miniconda3/envs/csci2470-project/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/csci2470-project/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def text_to_title(content, model=model, output_len=TITLE_SEQ_LEN):\n",
    "    \"\"\"Converts vectorized text to title\n",
    "    Arguments:\n",
    "        content - vectorized text\"\"\"\n",
    "    \n",
    "    start, end = tf.constant(title_word_index['<start>'], dtype=tf.int64), tf.constant(title_word_index['<end>'], dtype=tf.int64)\n",
    "    start = start[tf.newaxis]\n",
    "    end = end[tf.newaxis]\n",
    "    \n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    output_array = output_array.write(0, start)\n",
    "\n",
    "    for i in tf.range(output_len):\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        predictions = model([content[tf.newaxis], output], training=False)\n",
    "        \n",
    "        # Select the last token from the `seq_len` dimension.\n",
    "        predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "        predicted_id = tf.argmax(predictions, axis=2)\n",
    "\n",
    "        # Concatenate the `predicted_id` to the output which is given to the\n",
    "        # decoder as its input.\n",
    "        output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "        if predicted_id == end:\n",
    "            break\n",
    "        \n",
    "    output = output_array.stack().numpy().reshape(1,-1)\n",
    "    predicted_title = sentence_from_ind(output[0].tolist())\n",
    "    return predicted_title\n",
    "\n",
    "true_titles = []\n",
    "predicted_titles_original_bias = []\n",
    "predicted_titles_reverse_bias = []\n",
    "bleu_score_original_bias = []\n",
    "bleu_score_reverse_bias = []\n",
    "\n",
    "test_articles_len = len(test_content)\n",
    "\n",
    "for index in range(test_articles_len):\n",
    "    content_vec, reverse_content_vec, true_title = test_content_vec[index], test_reverse_content_vec[index], test_title[index]\n",
    "    predicted_title_original_bias = text_to_title(content_vec)\n",
    "    predicted_title_reverse_bias = text_to_title(reverse_content_vec)\n",
    "    \n",
    "    true_titles.append(true_title)\n",
    "    predicted_titles_original_bias.append(predicted_title_original_bias)\n",
    "    predicted_titles_reverse_bias.append(predicted_title_reverse_bias)\n",
    "    \n",
    "    bleu_score_original_bias.append(sentence_bleu([true_title.split()], predicted_title_original_bias.split(), \n",
    "                                    weights=(1,0,0,0)))\n",
    "    bleu_score_reverse_bias.append(sentence_bleu([true_title.split()], predicted_title_reverse_bias.split(), \n",
    "                                    weights=(1,0,0,0)))\n",
    "    \n",
    "    \n",
    "    if index%50==0:\n",
    "        print(index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce33b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=[true_titles, predicted_titles_original_bias, predicted_titles_reverse_bias,\n",
    "                        bleu_score_original_bias, bleu_score_reverse_bias,\n",
    "                        test_original_view[:test_articles_len], test_reverse_view[:test_articles_len]]).T\n",
    "df.columns = ['true_title','predicted_title_original_bias', 'predicted_title_reverse_bias',\n",
    "              'bleu_score_original_bias', 'bleu_score_reverse_bias',\n",
    "              'original_view', 'reverse_view']\n",
    "df['mean_bleu_score'] = (df['bleu_score_original_bias']+df['bleu_score_reverse_bias'])/2\n",
    "df.sort_values(by=['mean_bleu_score'],ascending=[False],inplace=True)\n",
    "df.to_csv('modelv2_results.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
