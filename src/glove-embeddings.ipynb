{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b114bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries needed for preprocessing\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "002d1a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload news article data \n",
    "with open('../data/nytfox_collate_v2.json','rb') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "# separate content and title data into separate lists \n",
    "content_arr = [item['content'] for item in data]\n",
    "title_arr = [item['title'] for item in data]\n",
    "num_samples = len(content_arr)\n",
    "\n",
    "# shuffle data for test and train sets \n",
    "np.random.seed(2470)\n",
    "idx = np.arange(0,num_samples)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "test_split = 0.2\n",
    "num_test_samples = int(test_split*num_samples)\n",
    "\n",
    "# create train and test sets for content and titles \n",
    "temp_content_arr = np.array(content_arr)[idx]\n",
    "temp_title_arr = np.array(title_arr)[idx]\n",
    "\n",
    "train_content = (temp_content_arr.tolist())[:-num_test_samples]\n",
    "test_content = (temp_content_arr.tolist())[-num_test_samples:]\n",
    "\n",
    "train_title = (temp_title_arr.tolist())[:-num_test_samples]\n",
    "test_title = (temp_title_arr.tolist())[-num_test_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eda5bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1490"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = [content for content in train_content if len(content.split())<256]\n",
    "len(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c708bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 15:11:50.229755: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# create vectorizers that map words to unique indexes in the vocab \n",
    "content_vectorizer = TextVectorization(max_tokens=100000, split='whitespace', output_mode='int', \n",
    "                                       standardize='lower_and_strip_punctuation',\n",
    "                                       output_sequence_length=256)\n",
    "\n",
    "title_vectorizer = TextVectorization(max_tokens=15000, split='whitespace', output_mode='int',\n",
    "                                     standardize='lower_and_strip_punctuation',\n",
    "                                     output_sequence_length=32)\n",
    "\n",
    "train_content_ds = tf.data.Dataset.from_tensor_slices(train_content).batch(128)\n",
    "train_title_ds = tf.data.Dataset.from_tensor_slices(train_title).batch(128)\n",
    "\n",
    "\n",
    "content_vectorizer.adapt(train_content_ds)\n",
    "title_vectorizer.adapt(train_title_ds)\n",
    "\n",
    "# create dictionaries that map unique words to indexes for both content and title data \n",
    "content_vocab = content_vectorizer.get_vocabulary()\n",
    "content_word_index = dict(zip(content_vocab, range(len(content_vocab))))\n",
    "\n",
    "title_vocab = title_vectorizer.get_vocabulary()\n",
    "title_word_index = dict(zip(title_vocab, range(len(title_vocab))))\n",
    "index_to_word = dict(zip(range(len(title_vocab)),title_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0895da2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# initalize the glove embedding space \n",
    "path_to_glove = 'glove.6B/glove.6B.100d.txt'\n",
    "embeddings_index = {}\n",
    "\n",
    "# dictionary with word: glove embedding \n",
    "with open(path_to_glove) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0538f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2470)\n",
    "embedding_size = 100\n",
    "\n",
    "# create randome vectors for the start and stop tokens \n",
    "start_embedding = np.random.normal(size=(embedding_size))\n",
    "stop_embedding = np.random.normal(size=(embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d20e0d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add positional_encodings to word_embeddings\n",
    "def positional_encoding(window_size, embedding_size):\n",
    "   \n",
    "    embedding_size = embedding_size/2\n",
    "    ## Generate a range of positions and depths \n",
    "    positions = np.arange(window_size)[:, np.newaxis]    # (seq, 1)\n",
    "    depths = np.arange(embedding_size)[np.newaxis, :]/embedding_size  # (1, depth)\n",
    "    ## Compute range of radians to take the sine and cosine of.\n",
    "    angle_rates = 1 / (10000**depths)               # (1, depth)\n",
    "    angle_rads = positions * angle_rates            # (pos, depth)\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1) \n",
    "    ## This serves as offset for the Positional Encoding\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# function to create embedded matrix for train and test data \n",
    "def create_embeddings(data, glove, max_emb_len, emb_size, dataset_name):\n",
    "    \n",
    "    # initalize embedding tensor for data \n",
    "    embedding_tensor = np.zeros(shape=(len(data),max_emb_len,emb_size))\n",
    "    \n",
    "    # embed each article in the data \n",
    "    for j, article in enumerate(data):\n",
    "        # get embedding for each word in data \n",
    "        for i, word in enumerate(article.split()):\n",
    "            # end embedding at maximum length \n",
    "            if i==max_emb_len:\n",
    "                break\n",
    "            # add start embedding as first row of each article block \n",
    "            # ARE WE MISSING THE FIRST WORD?\n",
    "            if i==0:\n",
    "                embedding_tensor[j][i] = start_embedding\n",
    "            # add stop embedding as last row of each article block \n",
    "            elif i==max_emb_len-1:\n",
    "                embedding_tensor[j][i] = stop_embedding\n",
    "            # grab the embedding of the word from glove \n",
    "            else:\n",
    "                embedding_tensor[j][i] = glove.get(word, np.zeros(emb_size))\n",
    "    \n",
    "    emb_with_pos = embedding_tensor + positional_encoding(max_emb_len,emb_size)\n",
    "    \n",
    "    print(f\"Shape of {dataset_name} embedding:\", emb_with_pos.shape)\n",
    "    \n",
    "    return emb_with_pos\n",
    "\n",
    "# function to create token labels for titles \n",
    "def create_token_labels(title_data,title_vectorizer, dataset):\n",
    "    \n",
    "    title_labels = []\n",
    "\n",
    "    for title in title_data:\n",
    "        title_labels.append(title_vectorizer(title).numpy())\n",
    "\n",
    "    title_labels = np.array(title_labels).reshape(len(title_data),-1,1)\n",
    "    \n",
    "    print(f'Shape of {dataset} token labels:', title_labels.shape)\n",
    "    \n",
    "    return title_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4b0662d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_content embedding: (17292, 256, 100)\n",
      "Shape of test_content embedding: (4323, 256, 100)\n",
      "Shape of train_title embedding: (17292, 32, 100)\n",
      "Shape of test_title embedding: (4323, 32, 100)\n"
     ]
    }
   ],
   "source": [
    "# use embedding function to create embedding matrix of all datasets \n",
    "train_content_emb = create_embeddings(train_content,embeddings_index,256,100,'train_content')\n",
    "test_content_emb = create_embeddings(test_content,embeddings_index,256,100,'test_content')\n",
    "\n",
    "train_title_emb = create_embeddings(train_title,embeddings_index,32,100,'train_title')\n",
    "test_title_emb = create_embeddings(test_title,embeddings_index,32,100,'test_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edc30335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train token labels: (17292, 32, 1)\n",
      "Shape of test token labels: (4323, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# use token function to create title labels for loss function\n",
    "train_title_labels = create_token_labels(train_title,title_vectorizer,'train')\n",
    "test_title_labels = create_token_labels(test_title,title_vectorizer,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d8f2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data as pickle file \n",
    "with open('train_content_embeddings.pkl','wb+') as f:\n",
    "    pickle.dump(train_content_emb, f)\n",
    "\n",
    "with open('train_title_embeddings.pkl','wb+') as f:\n",
    "    pickle.dump(train_title_emb, f)\n",
    "    \n",
    "with open('test_content_embeddings.pkl', 'wb+') as f:\n",
    "    pickle.dump(test_content_emb, f)\n",
    "\n",
    "with open('test_title_embeddings.pkl','wb+') as f:\n",
    "    pickle.dump(test_title_emb, f)\n",
    "    \n",
    "with open('train_title_labels.pkl','wb+') as f:\n",
    "    pickle.dump(train_title_labels, f)\n",
    "    \n",
    "with open('test_title_labels.pkl','wb+') as f:\n",
    "    pickle.dump(test_title_labels, f)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
