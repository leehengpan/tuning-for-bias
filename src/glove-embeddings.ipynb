{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b114bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "002d1a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/nytfox_collate_v2.json','rb') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "    \n",
    "content_arr = [item['content'] for item in data]\n",
    "title_arr = [item['title'] for item in data]\n",
    "num_samples = len(content_arr)\n",
    "\n",
    "np.random.seed(2470)\n",
    "idx = np.arange(0,num_samples)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "test_split = 0.2\n",
    "num_test_samples = int(test_split*num_samples)\n",
    "\n",
    "train_content = content_arr[:-num_test_samples]\n",
    "test_content = content_arr[-num_test_samples:]\n",
    "train_title = title_arr[:-num_test_samples]\n",
    "test_title = title_arr[-num_test_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c708bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 20:28:40.905272: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "content_vectorizer = TextVectorization(max_tokens=100000, split='whitespace', output_mode='int', \n",
    "                                       standardize='lower_and_strip_punctuation',\n",
    "                                       output_sequence_length=256)\n",
    "\n",
    "title_vectorizer = TextVectorization(max_tokens=15000, split='whitespace', output_mode='int',\n",
    "                                     standardize='lower_and_strip_punctuation',\n",
    "                                     output_sequence_length=32)\n",
    "\n",
    "train_content_ds = tf.data.Dataset.from_tensor_slices(train_content).batch(128)\n",
    "train_title_ds = tf.data.Dataset.from_tensor_slices(train_title).batch(128)\n",
    "\n",
    "\n",
    "content_vectorizer.adapt(train_content_ds)\n",
    "title_vectorizer.adapt(train_title_ds)\n",
    "\n",
    "content_vocab = content_vectorizer.get_vocabulary()\n",
    "content_word_index = dict(zip(content_vocab, range(len(content_vocab))))\n",
    "\n",
    "title_vocab = title_vectorizer.get_vocabulary()\n",
    "title_word_index = dict(zip(title_vocab, range(len(title_vocab))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b6aba9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
       "array([   2, 1208,  327, 4152,   15,   33,  877,  240,   69, 1444,    2,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0])>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_vectorizer(train_title[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0895da2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_glove = 'glove.6B/glove.6B.100d.txt'\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(path_to_glove) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "213d57ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 67966 words (32034 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(content_vocab) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "content_embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "\n",
    "for word, i in content_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        content_embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18bc042f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 13790 words (1210 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(title_vocab) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "title_embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "\n",
    "for word, i in title_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        title_embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0538f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(2470)\n",
    "embedding_size = title_embedding_matrix.shape[1]\n",
    "start_embedding = np.random.normal(size=(100))\n",
    "stop_embedding = np.random.normal(size=(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23fbca91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17292, 256, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_content_seq = 256\n",
    "train_content_embedding = np.zeros(shape=(len(train_content),train_content_seq,embedding_size))\n",
    "\n",
    "for j,article in enumerate(train_content):\n",
    "    for i,word in enumerate(article.split()):\n",
    "        if i==train_content_seq:\n",
    "            break\n",
    "        if i==0:\n",
    "            train_content_embedding[j][i] = start_embedding\n",
    "        elif i==train_content_seq-1:\n",
    "            train_content_embedding[j][i] = stop_embedding\n",
    "        else:\n",
    "            train_content_embedding[j][i] = embeddings_index.get(word, np.zeros(embedding_size))\n",
    "train_content_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caa99e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17292, 32, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_title_seq = 32\n",
    "train_title_embedding = np.zeros(shape=(len(train_title),train_title_seq,embedding_size))\n",
    "\n",
    "for j,title in enumerate(train_title):\n",
    "    for i,word in enumerate(title.split()):\n",
    "        if i==train_title_seq:\n",
    "            break\n",
    "        if i==0:\n",
    "            train_title_embedding[j][i] = start_embedding\n",
    "        elif i==train_content_seq-1:\n",
    "            train_title_embedding[j][i] = stop_embedding\n",
    "        else:\n",
    "            train_title_embedding[j][i] = embeddings_index.get(word, np.zeros(embedding_size))\n",
    "train_title_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "060ed187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4323, 256, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_content_seq = 256\n",
    "test_content_embedding = np.zeros(shape=(len(test_content),test_content_seq,embedding_size))\n",
    "\n",
    "for j,article in enumerate(test_content):\n",
    "    for i,word in enumerate(article.split()):\n",
    "        if i==test_content_seq:\n",
    "            break\n",
    "        if i==0:\n",
    "            test_content_embedding[j][i] = start_embedding\n",
    "        elif i==test_content_seq-1:\n",
    "            test_content_embedding[j][i] = stop_embedding\n",
    "        else:\n",
    "            test_content_embedding[j][i] = embeddings_index.get(word, np.zeros(embedding_size))\n",
    "test_content_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52c74be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4323, 32, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_title_seq = 32\n",
    "test_title_embedding = np.zeros(shape=(len(test_title),test_title_seq,embedding_size))\n",
    "\n",
    "for j,title in enumerate(test_title):\n",
    "    for i,word in enumerate(title.split()):\n",
    "        if i==test_title_seq:\n",
    "            break\n",
    "        if i==0:\n",
    "            test_title_embedding[j][i] = start_embedding\n",
    "        elif i==test_content_seq-1:\n",
    "            test_title_embedding[j][i] = stop_embedding\n",
    "        else:\n",
    "            test_title_embedding[j][i] = embeddings_index.get(word, np.zeros(embedding_size))\n",
    "test_title_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "612b6f13",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[   2],\n",
       "        [1208],\n",
       "        [ 327],\n",
       "        ...,\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]],\n",
       "\n",
       "       [[   2],\n",
       "        [  10],\n",
       "        [ 830],\n",
       "        ...,\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]],\n",
       "\n",
       "       [[   2],\n",
       "        [1641],\n",
       "        [ 144],\n",
       "        ...,\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[   2],\n",
       "        [  45],\n",
       "        [  84],\n",
       "        ...,\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]],\n",
       "\n",
       "       [[   2],\n",
       "        [  45],\n",
       "        [  84],\n",
       "        ...,\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]],\n",
       "\n",
       "       [[   2],\n",
       "        [  45],\n",
       "        [1003],\n",
       "        ...,\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_title_labels = []\n",
    "\n",
    "for title in train_title:\n",
    "    train_title_labels.append(title_vectorizer(title).numpy())\n",
    "\n",
    "train_title_labels = np.array(train_title_labels).reshape(len(train_title),-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0773df2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[    2],\n",
       "        [   45],\n",
       "        [   84],\n",
       "        ...,\n",
       "        [    0],\n",
       "        [    0],\n",
       "        [    0]],\n",
       "\n",
       "       [[    2],\n",
       "        [  938],\n",
       "        [ 1689],\n",
       "        ...,\n",
       "        [    0],\n",
       "        [    0],\n",
       "        [    0]],\n",
       "\n",
       "       [[    2],\n",
       "        [   16],\n",
       "        [10455],\n",
       "        ...,\n",
       "        [    0],\n",
       "        [    0],\n",
       "        [    0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[    2],\n",
       "        [ 3223],\n",
       "        [    1],\n",
       "        ...,\n",
       "        [    0],\n",
       "        [    0],\n",
       "        [    0]],\n",
       "\n",
       "       [[    2],\n",
       "        [ 3466],\n",
       "        [   25],\n",
       "        ...,\n",
       "        [    0],\n",
       "        [    0],\n",
       "        [    0]],\n",
       "\n",
       "       [[    2],\n",
       "        [13172],\n",
       "        [    1],\n",
       "        ...,\n",
       "        [    0],\n",
       "        [    0],\n",
       "        [    0]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_title_labels = []\n",
    "\n",
    "for title in test_title:\n",
    "    test_title_labels.append(title_vectorizer(title).numpy())\n",
    "test_title_labels = np.array(test_title_labels).reshape(len(test_title),-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adcaeddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('train_content_embeddings.pkl','wb+') as f:\n",
    "    pickle.dump(train_content_embedding, f)\n",
    "\n",
    "with open('train_title_embeddings.pkl','wb+') as f:\n",
    "    pickle.dump(train_title_embedding, f)\n",
    "    \n",
    "with open('test_content_embeddings.pkl', 'wb+') as f:\n",
    "    pickle.dump(test_content_embedding, f)\n",
    "\n",
    "with open('test_title_embeddings.pkl','wb+') as f:\n",
    "    pickle.dump(test_title_embedding, f)\n",
    "    \n",
    "with open('train_title_labels.pkl','wb+') as f:\n",
    "    pickle.dump(train_title_labels, f)\n",
    "    \n",
    "with open('test_title_labels.pkl','wb+') as f:\n",
    "    pickle.dump(test_title_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b0fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
