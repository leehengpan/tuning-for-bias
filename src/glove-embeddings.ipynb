{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94dca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/nytfox_collate_v2.json','rb') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "    \n",
    "content_arr = [item['content'] for item in data]\n",
    "title_arr = [item['title'] for item in data]\n",
    "num_samples = len(content_arr)\n",
    "\n",
    "np.random.seed(2470)\n",
    "idx = np.arange(0,num_samples)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "test_split = 0.2\n",
    "num_test_samples = int(test_split*num_samples)\n",
    "\n",
    "train_content = content_arr[:-num_test_samples]\n",
    "test_content = content_arr[-num_test_samples:]\n",
    "train_title = title_arr[:-num_test_samples]\n",
    "test_title = title_arr[-num_test_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad75674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "content_vectorizer = TextVectorization(max_tokens=100000, split='whitespace', output_mode='int', \n",
    "                                       standardize='lower_and_strip_punctuation',\n",
    "                                       output_sequence_length=256)\n",
    "\n",
    "title_vectorizer = TextVectorization(max_tokens=15000, split='whitespace', output_mode='int',\n",
    "                                     standardize='lower_and_strip_punctuation',\n",
    "                                     output_sequence_length=32)\n",
    "\n",
    "train_content_ds = tf.data.Dataset.from_tensor_slices(train_content).batch(128)\n",
    "train_title_ds = tf.data.Dataset.from_tensor_slices(train_title).batch(128)\n",
    "\n",
    "\n",
    "content_vectorizer.adapt(train_content_ds)\n",
    "title_vectorizer.adapt(train_title_ds)\n",
    "\n",
    "content_vocab = content_vectorizer.get_vocabulary()\n",
    "content_word_index = dict(zip(content_vocab, range(len(content_vocab))))\n",
    "\n",
    "title_vocab = title_vectorizer.get_vocabulary()\n",
    "title_word_index = dict(zip(title_vocab, range(len(title_vocab))))\n",
    "title_word_rev_index = dict(zip(range(len(title_vocab)), title_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5ce98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_vectorizer(['biden is here'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b8f1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_vectorizer(['biden is there'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a2e442",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_word_rev_index[685]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9795bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove = 'glove.6B/glove.6B.100d.txt'\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(path_to_glove) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(2470)\n",
    "embedding_size = title_embedding_matrix.shape[1]\n",
    "start_embedding = np.random.normal(size=(100))\n",
    "stop_embedding = np.random.normal(size=(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4948e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_content_seq = 256\n",
    "train_content_embedding = np.zeros(shape=(len(train_content),train_content_seq,embedding_size))\n",
    "\n",
    "for j,article in enumerate(train_content):\n",
    "    for i,word in enumerate(article.split()):\n",
    "        if i==train_content_seq:\n",
    "            break\n",
    "        if i==0:\n",
    "            train_content_embedding[j][i] = start_embedding\n",
    "        elif i==train_content_seq-1:\n",
    "            train_content_embedding[j][i] = stop_embedding\n",
    "        else:\n",
    "            train_content_embedding[j][i] = embeddings_index.get(word, np.zeros(embedding_size))\n",
    "train_content_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cb1354",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_title_seq = 32\n",
    "train_title_embedding = np.zeros(shape=(len(train_title),train_title_seq,embedding_size))\n",
    "\n",
    "for j,title in enumerate(train_title):\n",
    "    for i,word in enumerate(title.split()):\n",
    "        if i==train_title_seq:\n",
    "            break\n",
    "        if i==0:\n",
    "            train_title_embedding[j][i] = start_embedding\n",
    "        elif i==train_content_seq-1:\n",
    "            train_title_embedding[j][i] = stop_embedding\n",
    "        else:\n",
    "            train_title_embedding[j][i] = embeddings_index.get(word, np.zeros(embedding_size))\n",
    "train_title_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c969165",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_content_seq = 256\n",
    "test_content_embedding = np.zeros(shape=(len(test_content),test_content_seq,embedding_size))\n",
    "\n",
    "for j,article in enumerate(test_content):\n",
    "    for i,word in enumerate(article.split()):\n",
    "        if i==test_content_seq:\n",
    "            break\n",
    "        if i==0:\n",
    "            test_content_embedding[j][i] = start_embedding\n",
    "        elif i==test_content_seq-1:\n",
    "            test_content_embedding[j][i] = stop_embedding\n",
    "        else:\n",
    "            test_content_embedding[j][i] = embeddings_index.get(word, np.zeros(embedding_size))\n",
    "test_content_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f213488",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_title_seq = 32\n",
    "test_title_embedding = np.zeros(shape=(len(test_title),test_title_seq,embedding_size))\n",
    "\n",
    "for j,title in enumerate(test_title):\n",
    "    for i,word in enumerate(title.split()):\n",
    "        if i==test_title_seq:\n",
    "            break\n",
    "        if i==0:\n",
    "            test_title_embedding[j][i] = start_embedding\n",
    "        elif i==test_content_seq-1:\n",
    "            test_title_embedding[j][i] = stop_embedding\n",
    "        else:\n",
    "            test_title_embedding[j][i] = embeddings_index.get(word, np.zeros(embedding_size))\n",
    "test_title_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d71b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_title_labels = []\n",
    "\n",
    "for title in train_title[0]:\n",
    "    train_title_labels.append(title_vectorizer(title).numpy())\n",
    "\n",
    "train_title_labels = np.array(train_title_labels).reshape(len(train_title),-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef60793",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_title_labels = []\n",
    "\n",
    "for title in test_title:\n",
    "    test_title_labels.append(title_vectorizer(title).numpy())\n",
    "test_title_labels = np.array(test_title_labels).reshape(len(test_title),-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d85a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (title_vectorizer(train_title[0]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f5133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_word_rev_index[4152]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dddabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('train_content_embeddings.pkl','wb+') as f:\n",
    "    pickle.dump(train_content_embedding, f)\n",
    "\n",
    "with open('train_title_embeddings.pkl','wb+') as f:\n",
    "    pickle.dump(train_title_embedding, f)\n",
    "    \n",
    "with open('test_content_embeddings.pkl', 'wb+') as f:\n",
    "    pickle.dump(test_content_embedding, f)\n",
    "\n",
    "with open('test_title_embeddings.pkl','wb+') as f:\n",
    "    pickle.dump(test_title_embedding, f)\n",
    "    \n",
    "with open('train_title_labels.pkl','wb+') as f:\n",
    "    pickle.dump(train_title_labels, f)\n",
    "    \n",
    "with open('test_title_labels.pkl','wb+') as f:\n",
    "    pickle.dump(test_title_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62714ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_vectorizer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
