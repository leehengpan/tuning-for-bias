{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b114bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries needed for preprocessing\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "002d1a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>internet ad agency cites difficult outlook the...</td>\n",
       "      <td>technology briefing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when south koreas president yoon suk yeol touc...</td>\n",
       "      <td>japan hosts south koreas leader in tokyo as si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              corpus  \\\n",
       "0  internet ad agency cites difficult outlook the...   \n",
       "1  when south koreas president yoon suk yeol touc...   \n",
       "\n",
       "                                               label  \n",
       "0                                technology briefing  \n",
       "1  japan hosts south koreas leader in tokyo as si...  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload news article data \n",
    "with open('../data/nytfox_collate_v2.json','rb') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "# separate content and title data into separate lists \n",
    "content_arr = [item['content'] for item in data]\n",
    "title_arr = [item['title'] for item in data]\n",
    "num_samples = len(content_arr)\n",
    "\n",
    "# shuffle data for test and train sets \n",
    "np.random.seed(2470)\n",
    "idx = np.arange(0,num_samples)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "test_split = 0.2\n",
    "num_test_samples = int(test_split*num_samples)\n",
    "\n",
    "# create train and test sets for content and titles \n",
    "train_content = content_arr[:-num_test_samples]\n",
    "test_content = content_arr[-num_test_samples:]\n",
    "train_title = title_arr[:-num_test_samples]\n",
    "test_title = title_arr[-num_test_samples:]\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df[\"corpus\"] = test_content\n",
    "df[\"label\"] = test_title\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7d48675f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conservative groups rejoice after house passes major energy package'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_title[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e5c708bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# create vectorizers that map words to unique indexes in the vocab \n",
    "content_vectorizer = TextVectorization(max_tokens=100000, split='whitespace', output_mode='int', \n",
    "                                       standardize='lower_and_strip_punctuation',\n",
    "                                       output_sequence_length=256)\n",
    "\n",
    "title_vectorizer = TextVectorization(max_tokens=15000, split='whitespace', output_mode='int',\n",
    "                                     standardize='lower_and_strip_punctuation',\n",
    "                                     output_sequence_length=32)\n",
    "\n",
    "train_content_ds = tf.data.Dataset.from_tensor_slices(train_content).batch(128)\n",
    "train_title_ds = tf.data.Dataset.from_tensor_slices(train_title).batch(128)\n",
    "\n",
    "\n",
    "content_vectorizer.adapt(train_content_ds)\n",
    "title_vectorizer.adapt(train_title_ds)\n",
    "\n",
    "# create dictionaries that map unique words to indexes for both content and title data \n",
    "content_vocab = content_vectorizer.get_vocabulary()\n",
    "content_word_index = dict(zip(content_vocab, range(len(content_vocab))))\n",
    "\n",
    "title_vocab = title_vectorizer.get_vocabulary()\n",
    "title_word_index = dict(zip(title_vocab, range(len(title_vocab))))\n",
    "index_to_word = dict(zip(range(len(title_vocab)),title_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0895da2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Data format for word (\"the\"): [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n"
     ]
    }
   ],
   "source": [
    "# initalize the glove embedding space \n",
    "path_to_glove = 'glove.6B/glove.6B.100d.txt'\n",
    "embeddings_index = {}\n",
    "\n",
    "# dictionary with word: glove embedding \n",
    "with open(path_to_glove) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "print('Data format for word (\"the\"):',embeddings_index.get('the'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "213d57ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 67966 words (32034 misses)\n"
     ]
    }
   ],
   "source": [
    "# ASK IF HE USES THIS ##################\n",
    "\n",
    "# number of possible tokens is content_vocab + start and stop \n",
    "num_tokens = len(content_vocab) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "content_embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "\n",
    "for word, i in content_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        content_embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "18bc042f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 13790 words (1210 misses)\n"
     ]
    }
   ],
   "source": [
    "# do the same for titles \n",
    "num_tokens = len(title_vocab) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "title_embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "\n",
    "for word, i in title_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        title_embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0538f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2470)\n",
    "embedding_size = title_embedding_matrix.shape[1]\n",
    "\n",
    "# create randome vectors for the start and stop tokens \n",
    "start_embedding = np.random.normal(size=(embedding_size))\n",
    "stop_embedding = np.random.normal(size=(embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d20e0d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add positional_encodings to word_embeddings\n",
    "def positional_encoding(window_size, embedding_size):\n",
    "   \n",
    "    embedding_size = embedding_size/2\n",
    "    ## Generate a range of positions and depths \n",
    "    positions = np.arange(window_size)[:, np.newaxis]    # (seq, 1)\n",
    "    depths = np.arange(embedding_size)[np.newaxis, :]/embedding_size  # (1, depth)\n",
    "    ## Compute range of radians to take the sine and cosine of.\n",
    "    angle_rates = 1 / (10000**depths)               # (1, depth)\n",
    "    angle_rads = positions * angle_rates            # (pos, depth)\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1) \n",
    "    ## This serves as offset for the Positional Encoding\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# function to create embedded matrix for train and test data \n",
    "def create_embeddings(data, glove, max_emb_len, emb_size, dataset_name):\n",
    "    \n",
    "    # initalize embedding tensor for data \n",
    "    embedding_tensor = np.zeros(shape=(len(data),max_emb_len,emb_size))\n",
    "    \n",
    "    # embed each article in the data \n",
    "    for j, article in enumerate(data):\n",
    "        # get embedding for each word in data \n",
    "        for i, word in enumerate(article.split()):\n",
    "            # end embedding at maximum length \n",
    "            if i==max_emb_len:\n",
    "                break\n",
    "            # add start embedding as first row of each article block \n",
    "            # ARE WE MISSING THE FIRST WORD?\n",
    "            if i==0:\n",
    "                embedding_tensor[j][i] = start_embedding\n",
    "            # add stop embedding as last row of each article block \n",
    "            elif i==train_content_seq-1:\n",
    "                embedding_tensor[j][i] = stop_embedding\n",
    "            # grab the embedding of the word from glove \n",
    "            else:\n",
    "                embedding_tensor[j][i] = glove.get(word, np.zeros(emb_size))\n",
    "    \n",
    "    emb_with_pos = embedding_tensor + positional_encoding(max_emb_len,emb_size)\n",
    "    \n",
    "    print(f\"Shape of {dataset_name} embedding:\", emb_with_pos.shape)\n",
    "    \n",
    "    return emb_with_pos\n",
    "\n",
    "# function to create token labels for titles \n",
    "def create_token_labels(title_data,title_vectorizer, dataset):\n",
    "    \n",
    "    title_labels = []\n",
    "\n",
    "    for title in title_data:\n",
    "        title_labels.append(title_vectorizer(title).numpy())\n",
    "\n",
    "#     title_labels = np.array(title_labels).reshape(len(title_data),-1,1)\n",
    "    \n",
    "#     print(f'Shape of {dataset} token labels:', title_labels.shape)\n",
    "    \n",
    "    return title_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92b63a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f4b0662d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_content embedding: (17292, 256, 100)\n",
      "Shape of test_content embedding: (4323, 256, 100)\n",
      "Shape of train_title embedding: (17292, 32, 100)\n",
      "Shape of test_title embedding: (4323, 32, 100)\n"
     ]
    }
   ],
   "source": [
    "# use embedding function to create embedding matrix of all datasets \n",
    "train_content_emb = create_embeddings(train_content,embeddings_index,256,100,'train_content')\n",
    "test_content_emb = create_embeddings(test_content,embeddings_index,256,100,'test_content')\n",
    "\n",
    "train_title_emb = create_embeddings(train_title,embeddings_index,32,100,'train_title')\n",
    "test_title_emb = create_embeddings(test_title,embeddings_index,32,100,'test_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "edc30335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conservative groups rejoice after house passes major energy package\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use token function to create title labels for loss function\n",
    "print(train_title[0])\n",
    "train_title_labels = create_token_labels(train_title,title_vectorizer,'train')\n",
    "print(train_title_labels[0])\n",
    "# test_title_labels = create_token_labels(test_title,title_vectorizer,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "befb4ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1207,  326, 4152,   14,   32,  876,  239,   68, 1443,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_title_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a6875bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conservative groups rejoice after house passes major energy package                        '"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this = train_title_labels[0].reshape((32,))\n",
    "\n",
    "s = ''\n",
    "for tok in this:\n",
    "    s += index_to_word[tok]\n",
    "    s += ' '\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7a0bf131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conservative groups rejoice after house passes major energy package'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_title[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c6c79069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'technology briefing                               '"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this = test_title_labels[0].reshape((32,))\n",
    "s = ''\n",
    "for tok in this:\n",
    "    s += index_to_word[tok]\n",
    "    s += ' '\n",
    "\n",
    "s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5d8f2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data as pickle file \n",
    "with open('train_content_embeddings.pkl','wb+') as f:\n",
    "    pickle.dump(train_content_emb, f)\n",
    "\n",
    "with open('train_title_embeddings.pkl','wb+') as f:\n",
    "    pickle.dump(train_title_emb, f)\n",
    "    \n",
    "with open('test_content_embeddings.pkl', 'wb+') as f:\n",
    "    pickle.dump(test_content_emb, f)\n",
    "\n",
    "with open('test_title_embeddings.pkl','wb+') as f:\n",
    "    pickle.dump(test_title_emb, f)\n",
    "    \n",
    "with open('train_title_labels.pkl','wb+') as f:\n",
    "    pickle.dump(train_title_labels, f)\n",
    "    \n",
    "with open('test_title_labels.pkl','wb+') as f:\n",
    "    pickle.dump(test_title_labels, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a1e7490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('index_to_word.pkl','wb+') as f:\n",
    "    pickle.dump(index_to_word, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "20f1cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN DELETE ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "23fbca91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train content embeddings: (17292, 256, 100)\n"
     ]
    }
   ],
   "source": [
    "# max length of content for embeddings\n",
    "train_content_seq = 256\n",
    "train_content_embedding = np.zeros(shape=(len(train_content),train_content_seq,embedding_size))\n",
    "\n",
    "# embedd tokens in each article up until 256 tokens \n",
    "for j,article in enumerate(train_content):\n",
    "    for i,word in enumerate(article.split()):\n",
    "        # end embedding at maximum length \n",
    "        if i==train_content_seq:\n",
    "            break\n",
    "        # add start embedding as first row of each article block \n",
    "        if i==0:\n",
    "            train_content_embedding[j][i] = start_embedding\n",
    "        # add stop embedding as last row of each article block \n",
    "        elif i==train_content_seq-1:\n",
    "            train_content_embedding[j][i] = stop_embedding\n",
    "        # grab the embedding of the word from glove \n",
    "        else:\n",
    "            train_content_embedding[j][i] = embeddings_index.get(word, np.zeros(embedding_size))\n",
    "print('Shape of train content embeddings:',train_content_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "caa99e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17292, 32, 100)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get embeddings for \n",
    "train_title_seq = 32\n",
    "train_title_embedding = np.zeros(shape=(len(train_title),train_title_seq,embedding_size))\n",
    "\n",
    "for j,title in enumerate(train_title):\n",
    "    for i,word in enumerate(title.split()):\n",
    "        if i==train_title_seq:\n",
    "            break\n",
    "        if i==0:\n",
    "            train_title_embedding[j][i] = start_embedding\n",
    "        elif i==train_content_seq-1:\n",
    "            train_title_embedding[j][i] = stop_embedding\n",
    "        else:\n",
    "            train_title_embedding[j][i] = embeddings_index.get(word, np.zeros(embedding_size))\n",
    "train_title_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "060ed187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4323, 256, 100)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_content_seq = 256\n",
    "test_content_embedding = np.zeros(shape=(len(test_content),test_content_seq,embedding_size))\n",
    "\n",
    "for j,article in enumerate(test_content):\n",
    "    for i,word in enumerate(article.split()):\n",
    "        if i==test_content_seq:\n",
    "            break\n",
    "        if i==0:\n",
    "            test_content_embedding[j][i] = start_embedding\n",
    "        elif i==test_content_seq-1:\n",
    "            test_content_embedding[j][i] = stop_embedding\n",
    "        else:\n",
    "            test_content_embedding[j][i] = embeddings_index.get(word, np.zeros(embedding_size))\n",
    "test_content_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "52c74be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4323, 32, 100)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_title_seq = 32\n",
    "test_title_embedding = np.zeros(shape=(len(test_title),test_title_seq,embedding_size))\n",
    "\n",
    "for j,title in enumerate(test_title):\n",
    "    for i,word in enumerate(title.split()):\n",
    "        if i==test_title_seq:\n",
    "            break\n",
    "        if i==0:\n",
    "            test_title_embedding[j][i] = start_embedding\n",
    "        elif i==test_content_seq-1:\n",
    "            test_title_embedding[j][i] = stop_embedding\n",
    "        else:\n",
    "            test_title_embedding[j][i] = embeddings_index.get(word, np.zeros(embedding_size))\n",
    "test_title_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e0773df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_title_labels = []\n",
    "\n",
    "for title in test_title:\n",
    "    test_title_labels.append(title_vectorizer(title).numpy())\n",
    "test_title_labels = np.array(test_title_labels).reshape(len(test_title),-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6736f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
