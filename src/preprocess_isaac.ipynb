{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e768fc",
   "metadata": {},
   "source": [
    "<h1><center>Data Preprocessing - BART Base</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc0db66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 19:46:02.337449: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-19 19:46:15.263553: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFBartModel.\n",
      "\n",
      "All the layers of TFBartModel were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# import needed libraries and checkpoint models \n",
    "import json\n",
    "import tensorflow as tf\n",
    "from transformers import BartTokenizer, TFBartModel\n",
    "\n",
    "# initalize the tokenizer and model for embeddings \n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "model = TFBartModel.from_pretrained('facebook/bart-base')\n",
    "\n",
    "\n",
    "# Load data from JSON file\n",
    "with open('../data/nytfox_collate_content_quick_v1.json', 'r') as f:\n",
    "    articles = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27d7f0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NYT data from JSON file\n",
    "with open('../data/nyt_collate_content_v1.json', 'r') as f:\n",
    "    nyt_articles = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e508726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mandate the maximum size of the text and title embeddings \n",
    "max_input_length = 512\n",
    "max_target_length = 30\n",
    "\n",
    "# function to embed words from the articles and titles \n",
    "def embed_data(article):\n",
    "    \n",
    "    # dictionary to save tokenized article\n",
    "    article_tokens = {}\n",
    "    \n",
    "    # tokenize the contents of an article \n",
    "    content_tokens = tokenizer(\n",
    "        article[\"content\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    # save tokenized content and masks to dict\n",
    "    article_tokens['content'] = content_tokens['input_ids']\n",
    "    article_tokens['content_mask'] = content_tokens['attention_mask']\n",
    "    \n",
    "    # tokenize the title of an article\n",
    "    title_tokens = tokenizer(\n",
    "        article[\"title\"], max_length=max_target_length, truncation=True\n",
    "    )\n",
    "    \n",
    "    # save tokenized title and masks to dict\n",
    "    article_tokens[\"title\"] = title_tokens[\"input_ids\"]\n",
    "    article_tokens['title_mask'] = title_tokens['attention_mask']\n",
    "    \n",
    "    \n",
    "    outputs = model(article_tokens['content'])\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    \n",
    "    print(last_hidden_states)\n",
    "    return article_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67797fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBartModel.\n",
      "\n",
      "All the layers of TFBartModel were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBartModel, BartTokenizer\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "model = TFBartModel.from_pretrained('facebook/bart-base')\n",
    "\n",
    "def embed_data(article):\n",
    "    article_tokens = {}\n",
    "\n",
    "    # tokenize the contents of an article\n",
    "    content_tokens = tokenizer(\n",
    "        article[\"content\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "    # save tokenized content and masks to dict\n",
    "    article_tokens['content'] = content_tokens['input_ids']\n",
    "    article_tokens['content_mask'] = content_tokens['attention_mask']\n",
    "\n",
    "    # tokenize the title of an article\n",
    "    title_tokens = tokenizer(\n",
    "        article[\"title\"],\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "    # save tokenized title and masks to dict\n",
    "    article_tokens[\"title\"] = title_tokens[\"input_ids\"]\n",
    "    article_tokens['title_mask'] = title_tokens['attention_mask']\n",
    "\n",
    "    # get the embeddings\n",
    "    with tf.device('/cpu:0'):\n",
    "        content_embed = model(content_tokens['input_ids'], attention_mask=content_tokens['attention_mask'])[0]\n",
    "        title_embed = model(title_tokens['input_ids'], attention_mask=title_tokens['attention_mask'])[0]\n",
    "\n",
    "    # save the embeddings to dict\n",
    "    article_tokens['content_embed'] = content_embed\n",
    "    article_tokens['title_embed'] = title_embed\n",
    "\n",
    "    return article_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ca86666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to save embedded articles \n",
    "embedded_articles = []\n",
    "\n",
    "# embed each article and title \n",
    "for article in articles[0:1]:\n",
    "    \n",
    "    # create tokenized dictionary \n",
    "    article_tokens = embed_data(article)\n",
    "    \n",
    "    # save to master list \n",
    "    embedded_articles.append(article_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50511783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': <tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
       "  array([[    0, 39679,  4014,  5121,  7481,    35,    83,  3782,     9,\n",
       "            753,  3354,  8973,  1134,  5417,   296,    71,     5,   446,\n",
       "           1595,     5,  9978,  2169, 32125,  1783,     8,   373,    15,\n",
       "              5,  1112,     7, 21790, 11511,     5,  2309,     4, 50118,\n",
       "          50118,   133,  1134,     6,   669,    30,  1791,    13, 30052,\n",
       "           1571,     6, 17407,     5,  1087,    25,    22,  1245,  6920,\n",
       "           2309,    60,  7594,    24,    74,  3231,   270, 15478,    18,\n",
       "           2147,  4026,     8,  9128,  3575,  2680,     4,    20,  2309,\n",
       "           1171,  7668,     7,  2501,  1897,  1007,   931,     6,  8373,\n",
       "             62,  2008, 10026,  1787,  9781,     8,  3114,  2210, 19289,\n",
       "           2074,     4, 50118, 50118,   113,   170, 19781,     5,   446,\n",
       "             13,  3133,    42, 10043,  2309,     7, 26845, 11685,  1007,\n",
       "          18225,    60,     5,  1134,    26,    11,    10,   445,     7,\n",
       "           2063,   491,  6282,     4,    22,  7605,   183,    65,     9,\n",
       "             42,  4237,     6,   270, 15478,    34,  1690, 35276,     5,\n",
       "           1252,     8,  1822,    14,  2423,    84,   247,    19,     5,\n",
       "           2016,   865,     9,   168, 22386,     6,  2556,     6,     8,\n",
       "             81, 40757,    72, 50118, 50118,   113, 15085,    32,  2746,\n",
       "              5,   425,    13,     5, 15478, 29781, 27748,   997,    15,\n",
       "           1007,    60,     5,   445,  1143,     4,    22, 38657, 23700,\n",
       "           2680,    16,  6437,  1232,     7,  1930,    55,    15,   960,\n",
       "             31,  1123,     7, 20279,     6,     8,     5,   239,  1007,\n",
       "            850,  1428,  2680,    32,  3022,     5,  3912,  1425,  8195,\n",
       "              5, 11111,    72, 50118, 50118, 38543,  8640,  1889,   132,\n",
       "          14010,  1301,  2796,  8837,  5061,  3888, 23098,  4979, 16821,\n",
       "          43743,  6557,   108,  8883, 28142,  1862, 13245,  2076, 21661,\n",
       "          44561, 14426, 50118, 50118,  1121,  1285,     7,  1791,    13,\n",
       "          30052,  1571,     6,  8784,  5828,     6,  7978, 26025,     6,\n",
       "          17638,  7710,   470,  7978,     6,   496,  6394, 26259,  1332,\n",
       "              8,     5,   470,  2169,  6035,    58,   233,     9,     5,\n",
       "           3782, 16765,     5,   900,     4, 50118, 50118, 36342,    63,\n",
       "            762,  1797,     6,     5,  2309,    74,  7677,    92,  2556,\n",
       "             15,  1632,  1123,  2112,     6,  1306,  1675,   681,     8,\n",
       "           1123, 17847,    15,   752,  8952,     8,  5794,     6,  7677,\n",
       "          19289, 14252,     7,  4116,   709,     6,  4615,  1902, 30501,\n",
       "          41983,  3478,  1330,     7,  1632,  1123,   931,     8, 12397,\n",
       "              5,  1460,    13,  4481,   451,     7,  5242,    10,  3651,\n",
       "            121,     4,   104,     4,  2008, 10026,  1787,  3206,     4,\n",
       "          50118, 50118,   387,  2688,  2796,  4516, 24765,   108,   104,\n",
       "          22543,  5121, 35923,   725, 20245,  1480,  7205, 10296,   347,\n",
       "           1723, 18342,  5267, 27291,   289,  5969, 21770,  4729, 32870,\n",
       "              6,   211,  5216, 13216,  2076,  5289, 34072,  1723,  4248,\n",
       "          15421,   725,  1723,     6, 10649, 21260,  2685, 47612, 50118,\n",
       "          50118,   113, 15085,   240,  1148,     7,  1413,    62,     7,\n",
       "              5, 15478, 29781, 27748,  2680,  1766,  4026,     8,  7213,\n",
       "              5,  1880,   626,    30, 12658,  1290,  1447,  1007,  1986,\n",
       "             60,     5,  3782,   355,     4,    22,   565,  1630,  4113,\n",
       "           9078,     9,    20,  9978,  2169, 32125,  1783,  4863,    41,\n",
       "            505,  1149,  1706,  6477,     5,   239,  1042,  1791,    32,\n",
       "           2114,     4,    20,  9978,  2169, 32125,  1783,    40,  4615,\n",
       "           1902,     5, 19289,   609,     7,  1769,  1349,     5,  1663,\n",
       "              9,  1007,  2112,  1377,  1328,    84,  1226,    72, 50118,\n",
       "          50118,   113,   133,   563,    40,  8189,   823,    68,   541,\n",
       "            325,     9, 11827,    12, 11856,   865,  4518,     7,  7557,\n",
       "           6813,  4510,     8,  6024,  1134,    60,     5,   445,  1143,\n",
       "              4,    22,   243,    40,  5258,  7926,     7,  1007,   931,\n",
       "              7,  1157,   451,     7,   342,    82,     7,     2]],\n",
       "        dtype=int32)>,\n",
       "  'content_mask': <tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
       "  array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1]], dtype=int32)>,\n",
       "  'title': <tf.Tensor: shape=(1, 30), dtype=int32, numpy=\n",
       "  array([[    0, 43381,  1134, 38818,    71,   446,  3974,   538,  1007,\n",
       "           3737,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1]], dtype=int32)>,\n",
       "  'title_mask': <tf.Tensor: shape=(1, 30), dtype=int32, numpy=\n",
       "  array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n",
       "  'content_embed': <tf.Tensor: shape=(1, 512, 768), dtype=float32, numpy=\n",
       "  array([[[ 2.6503448 ,  3.2016244 ,  1.3388029 , ...,  2.2058847 ,\n",
       "            0.3312998 , -0.7049581 ],\n",
       "          [-0.8145513 , -0.57293844, -1.6074227 , ..., -1.2925094 ,\n",
       "            1.9251825 ,  0.19470358],\n",
       "          [ 1.4512795 , -0.10057461, -0.5397182 , ..., -2.602287  ,\n",
       "           -0.05150816,  0.66684645],\n",
       "          ...,\n",
       "          [ 1.6137371 ,  0.6078762 ,  1.4367299 , ...,  0.5003481 ,\n",
       "            1.8043325 , -1.1023154 ],\n",
       "          [ 0.48527208,  0.02323312, -1.3962259 , ...,  0.01635098,\n",
       "            1.1910217 , -0.7607111 ],\n",
       "          [-1.0391499 , -0.6451527 ,  0.4608687 , ...,  0.95216405,\n",
       "            0.7718486 ,  0.71542233]]], dtype=float32)>,\n",
       "  'title_embed': <tf.Tensor: shape=(1, 30, 768), dtype=float32, numpy=\n",
       "  array([[[ 3.0996196 ,  2.6386685 ,  0.9838944 , ...,  1.1745316 ,\n",
       "            0.09106122, -0.4263556 ],\n",
       "          [ 0.23541631, -0.22208616, -0.7138883 , ...,  0.81462216,\n",
       "           -0.71179074,  1.0002047 ],\n",
       "          [ 0.26455003,  1.3169448 ,  2.3153207 , ..., -0.16263297,\n",
       "            1.5328352 ,  1.2701865 ],\n",
       "          ...,\n",
       "          [-1.4409972 , -0.28482544,  0.3908698 , ..., -0.7881518 ,\n",
       "            3.360143  ,  0.7263551 ],\n",
       "          [-1.3626966 , -0.24425822,  0.33304182, ..., -1.0417334 ,\n",
       "            3.555593  ,  0.6854826 ],\n",
       "          [-1.3993466 , -0.34196088,  0.30228293, ..., -1.0507405 ,\n",
       "            3.5142808 ,  0.70126563]]], dtype=float32)>}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "793a694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to save embedded articles \n",
    "embedded_articles = []\n",
    "\n",
    "# embed each article and title \n",
    "for article in nyt_articles:\n",
    "    \n",
    "    # create tokenized dictionary \n",
    "    article_tokens = embed_data(article)\n",
    "    \n",
    "    # save to master list \n",
    "    embedded_articles.append(article_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "71c04f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'B',\n",
       " 'iden',\n",
       " 'Ġnominee',\n",
       " 'Ġcoordinated',\n",
       " 'Ġdark',\n",
       " 'Ġmoney',\n",
       " 'Ġclimate',\n",
       " 'Ġnuisance',\n",
       " 'Ġlawsuits',\n",
       " 'Ġinvolving',\n",
       " 'ĠLeonardo',\n",
       " 'ĠDi',\n",
       " 'Cap',\n",
       " 'rio',\n",
       " '</s>']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(embedded_articles[1]['title'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
